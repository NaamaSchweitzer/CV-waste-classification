{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+FH34dEVVCinuEogXzLru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaamaSchweitzer/CV-waste-classification/blob/main/CV_Waste_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIYrGJvA9NAQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf82129"
      },
      "source": [
        "### Kaggle Setup and Dataset Download\n",
        "\n",
        "This section will guide you through setting up Kaggle in your Colab environment and downloading the `alyyan/trash-detection` dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8741cb1c",
        "outputId": "13d3ceef-3f5d-43eb-ed2f-4209b0171fbb"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install the Kaggle API client\n",
        "!pip install kaggle\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "062a03be"
      },
      "source": [
        "#### Authenticate with Kaggle\n",
        "\n",
        "To download datasets from Kaggle, you need to authenticate using your Kaggle API token. Follow these steps:\n",
        "\n",
        "1.  Go to your Kaggle account page (https://www.kaggle.com/your-username/account).\n",
        "2.  Scroll down to the 'API' section and click 'Create New API Token'. This will download a `kaggle.json` file.\n",
        "3.  In Colab, click on the ðŸ”‘ icon (Secrets) in the left sidebar. Add a new secret named `KAGGLE_USERNAME` for your Kaggle username and `KAGGLE_KEY` for your Kaggle API key (from the `kaggle.json` file).\n",
        "\n",
        "Alternatively, you can upload the `kaggle.json` file directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc06a608",
        "outputId": "6fb9cce4-87bb-4f81-9998-ec9910f254be"
      },
      "source": [
        "# Option 1: Using Colab Secrets (Recommended)\n",
        "from google.colab import userdata\n",
        "\n",
        "# Ensure these secrets are set in Colab's Secrets manager\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Option 2: Uploading kaggle.json directly (uncomment and run if not using secrets)\n",
        "# from google.colab import files\n",
        "# files.upload() # This will prompt you to upload the kaggle.json file\n",
        "\n",
        "# Create .kaggle directory if it doesn't exist and move the file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json # Set permissions\n",
        "\n",
        "print(\"Kaggle authentication setup complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle authentication setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048d87df"
      },
      "source": [
        "#### Download the Dataset\n",
        "\n",
        "Now we will download the `trash-detection` dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "398c42d6",
        "outputId": "e4ddef24-e90d-4b8a-d15e-d1e3da4610c2"
      },
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d alyyan/trash-detection\n",
        "\n",
        "# List the downloaded file(s) to verify\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/alyyan/trash-detection\n",
            "License(s): MIT\n",
            "Downloading trash-detection.zip to /content\n",
            " 92% 1.13G/1.23G [00:07<00:02, 42.0MB/s]\n",
            "100% 1.23G/1.23G [00:08<00:00, 163MB/s] \n",
            "kaggle.json  sample_data  trash-detection.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1e130a9"
      },
      "source": [
        "#### Unzip the Dataset\n",
        "\n",
        "The dataset is typically downloaded as a zip file. Let's extract its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8f9c32",
        "outputId": "5bf300a9-683c-4807-ff05-c53ea8f64a8f"
      },
      "source": [
        "# Unzip the dataset (assuming it's a .zip file)\n",
        "!unzip -q trash-detection.zip -d trash-detection\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "!ls trash-detection\n",
        "\n",
        "print(\"Dataset downloaded and extracted successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset\n",
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f610998a"
      },
      "source": [
        "# Task\n",
        "Analyze the structure of the `trash-detection/Dataset` directory, read the `trash-detection/Dataset/data.yaml` file to extract class names, count the total instances and analyze the class balance by processing the label files within the dataset, and finally provide a comprehensive summary of the dataset including its overall size, detected classes, and the distribution of instances across these classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd81a6e"
      },
      "source": [
        "## Explore Dataset Structure\n",
        "\n",
        "### Subtask:\n",
        "List the contents of the `trash-detection/Dataset` directory to understand how the images and labels are organized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3215e6"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand how the images and labels are organized, I will list the contents of the `trash-detection/Dataset` directory using a shell command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc1b073a",
        "outputId": "8a859851-3e50-4dc0-fcc2-dcfee0b09de3"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the 'trash-detection/Dataset' directory\n",
        "!ls -F trash-detection/Dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.yaml  images/  labels/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5747c5d5"
      },
      "source": [
        "## Read Data Configuration\n",
        "\n",
        "### Subtask:\n",
        "Load and parse the `trash-detection/Dataset/data.yaml` file to extract class names and any other relevant dataset metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1b6c12"
      },
      "source": [
        "**Reasoning**:\n",
        "Before parsing the YAML file, the `pyyaml` library needs to be installed, as it's not part of the standard Python library and was not installed in previous steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e01de6b9",
        "outputId": "0b1ffd3f-9396-4f65-a533-70e8623ca597"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install the PyYAML library to parse YAML files\n",
        "!pip install pyyaml\n",
        "\n",
        "print(\"PyYAML installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "PyYAML installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7d217a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that PyYAML is installed, I will import the `yaml` library, construct the path to `data.yaml`, open and parse it using `yaml.safe_load`, and then extract the class names as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b5c94bd",
        "outputId": "e62bae38-491b-41f0-b694-f363cf734308"
      },
      "source": [
        "import yaml\n",
        "\n",
        "# Construct the full path to the data.yaml file\n",
        "data_yaml_path = 'trash-detection/Dataset/data.yaml'\n",
        "\n",
        "# Initialize variables\n",
        "class_names = []\n",
        "\n",
        "# Open and read the data.yaml file\n",
        "with open(data_yaml_path, 'r') as file:\n",
        "    data = yaml.safe_load(file)\n",
        "\n",
        "# Extract class names\n",
        "if 'names' in data:\n",
        "    class_names = data['names']\n",
        "elif 'nc' in data and 'names' in data:\n",
        "    class_names = data['names']\n",
        "\n",
        "print(f\"Dataset metadata loaded from: {data_yaml_path}\")\n",
        "print(f\"Detected classes (nc): {len(class_names)}\")\n",
        "print(f\"Class names: {class_names}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset metadata loaded from: trash-detection/Dataset/data.yaml\n",
            "Detected classes (nc): 4\n",
            "Class names: ['dirt', 'liquid', 'marks', 'trash']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "643d0856"
      },
      "source": [
        "## Count Total Instances and Analyze Class Balance\n",
        "\n",
        "### Subtask:\n",
        "Identify the label files (e.g., .txt files) within the dataset and count the total number of labels. Then, iterate through these label files to count the occurrences of each class, providing a breakdown of the class balance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89444eda"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the instructions to identify label files, count total instances, and analyze class balance by iterating through the label directory, reading each label file, extracting class IDs, and then aggregating the counts for each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923a0b54"
      },
      "source": [
        "# Task\n",
        "Count the total number of labels in `trash-detection/Dataset/labels`, and then parse the class ID from each line in these label files (which are in YOLO annotation format) to count the occurrences of each class and analyze the class balance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b93b2709"
      },
      "source": [
        "## Count Total Instances and Analyze Class Balance\n",
        "\n",
        "### Subtask:\n",
        "Identify the label files (e.g., .txt files) within the dataset, which are in YOLO annotation format. Iterate through these label files, parse the class ID from each line (the first element), and count the total number of instances and the occurrences of each class to provide a breakdown of the class balance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a3bafe6"
      },
      "source": [
        "**Reasoning**:\n",
        "To count total instances and analyze class balance, I will define the labels directory, initialize counters, iterate through all .txt label files, extract class IDs from each line, and aggregate the counts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bc2ff07",
        "outputId": "b3262cd7-a685-4d79-eeed-09be6773057b"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the labels directory\n",
        "labels_dir = './trash-detection/Dataset/labels/train'\n",
        "\n",
        "# Initialize an empty dictionary to store class counts\n",
        "# and a variable to store the total number of instances\n",
        "class_counts = {i: 0 for i in range(len(class_names))}\n",
        "total_instances = 0\n",
        "\n",
        "# List all files in the labels directory\n",
        "all_label_files = os.listdir(labels_dir)\n",
        "\n",
        "# Filter for .txt files (YOLO label files)\n",
        "label_files = [f for f in all_label_files if f.endswith('.txt')]\n",
        "\n",
        "print(f\"Processing {len(label_files)} label files...\")\n",
        "\n",
        "# Iterate through each identified label file\n",
        "for filename in label_files:\n",
        "    filepath = os.path.join(labels_dir, filename)\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            # Increment total instances for each line (each bounding box)\n",
        "            total_instances += 1\n",
        "\n",
        "            # Extract class ID (first element in the line)\n",
        "            try:\n",
        "                class_id = int(line.strip().split(' ')[0])\n",
        "                # Increment the count for this class ID\n",
        "                class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not parse class ID in file {filename}, line: {line.strip()}. Error: {e}\")\n",
        "            except IndexError as e:\n",
        "                print(f\"Warning: Empty line or unexpected format in file {filename}, line: {line.strip()}. Error: {e}\")\n",
        "\n",
        "print(\"\\n--- Dataset Class Analysis ---\")\n",
        "print(f\"Total instances (bounding boxes detected): {total_instances}\")\n",
        "print(\"Class distribution:\")\n",
        "\n",
        "# Print class names with their counts\n",
        "for class_id, count in class_counts.items():\n",
        "    if class_id < len(class_names):\n",
        "        class_name = class_names[class_id]\n",
        "        print(f\"  - {class_name} (ID {class_id}): {count} instances\")\n",
        "    else:\n",
        "        print(f\"  - Unknown Class (ID {class_id}): {count} instances (not in data.yaml)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1229 label files...\n",
            "\n",
            "--- Dataset Class Analysis ---\n",
            "Total instances (bounding boxes detected): 7900\n",
            "Class distribution:\n",
            "  - dirt (ID 0): 1418 instances\n",
            "  - liquid (ID 1): 494 instances\n",
            "  - marks (ID 2): 3373 instances\n",
            "  - trash (ID 3): 2615 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0a7e4d"
      },
      "source": [
        "## Summarize Dataset Information\n",
        "\n",
        "### Subtask:\n",
        "Present a summary of the dataset, including its overall size, the detected classes, and the distribution of instances across these classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a19ad52"
      },
      "source": [
        "### Dataset Summary\n",
        "\n",
        "This section provides a summary of the `trash-detection` dataset, based on the analysis of its structure and label files.\n",
        "\n",
        "-   **Overall Dataset Size:**\n",
        "    -   Total number of label files processed: **1229**\n",
        "    -   Total number of instances (bounding boxes) detected: **7900**\n",
        "\n",
        "-   **Detected Classes:**\n",
        "    -   The dataset contains **4** distinct classes.\n",
        "    -   Class Names: `['dirt', 'liquid', 'marks', 'trash']`\n",
        "\n",
        "-   **Class Distribution (Instance Count per Class):**\n",
        "    -   `dirt`: 1418 instances\n",
        "    -   `liquid`: 494 instances\n",
        "    -   `marks`: 3373 instances\n",
        "    -   `trash`: 2615 instances\n",
        "\n",
        "This summary indicates that the `marks` class has the highest number of instances, followed by `trash`, `dirt`, and `liquid` having the least."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67e4978c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a comprehensive description of the dataset based on the analysis, including its size, columns, and class distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9baf86df"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The total number of label files processed is 1229. The class IDs were parsed from each line in these files, revealing 7900 total instances (bounding boxes) across 4 distinct classes. The occurrences of each class are: `dirt` with 1418 instances, `liquid` with 494 instances, `marks` with 3373 instances, and `trash` with 2615 instances. This analysis indicates an imbalanced class distribution.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The dataset comprises 1229 label files.\n",
        "*   A total of 7900 instances (bounding boxes) were detected across all label files.\n",
        "*   There are 4 distinct classes: `dirt`, `liquid`, `marks`, and `trash`.\n",
        "*   The class distribution is as follows:\n",
        "    *   `dirt`: 1418 instances\n",
        "    *   `liquid`: 494 instances\n",
        "    *   `marks`: 3373 instances\n",
        "    *   `trash`: 2615 instances\n",
        "*   The `marks` class has the highest number of instances (3373), while the `liquid` class has the fewest (494), indicating a significant class imbalance.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The significant class imbalance, particularly the low number of instances for `liquid` (494) compared to `marks` (3373), could lead to biased model performance.\n",
        "*   Consider data augmentation techniques or oversampling for minority classes (`liquid`, `dirt`) and potentially undersampling for majority classes (`marks`, `trash`) to balance the dataset for improved model training.\n"
      ]
    }
  ]
}